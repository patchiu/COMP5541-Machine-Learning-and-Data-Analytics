{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a19cc5-cfc1-484f-b120-89fea275138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30aa36f2-7b24-415d-ba20-d2a1737cff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNet_ont_hot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet_ont_hot, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 15)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(15, 10)\n",
    "        self.type = 'MLP'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class NeuralNet_binary_encoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet_binary_encoding, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 15)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(15, 4)\n",
    "        self.type = 'MLP'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "    \n",
    "class NeuralNet_binary_encoding_one_more_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet_binary_encoding_one_more_layer, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 15)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(15, 4)\n",
    "        self.fc3 = nn.Linear(4, 4)\n",
    "        self.type = 'MLP'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        out = self.fc3(x)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def create_dataloader_one_hot():\n",
    "    \n",
    "    # MNIST dataset\n",
    "    train_dataset = torchvision.datasets.MNIST(root='/',\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            target_transform=lambda y: torch.zeros(10,dtype=torch.float).scatter_(0, torch.tensor(y),value=1))\n",
    "\n",
    "    test_dataset = torchvision.datasets.MNIST(root='/',\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            target_transform=lambda y: torch.zeros(10,dtype=torch.float).scatter_(0, torch.tensor(y),value=1))\n",
    "\n",
    "    # Data loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                            batch_size=64,\n",
    "                                            shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=64,\n",
    "                                            shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "def create_dataloader_binary_encoding():\n",
    "    \n",
    "    # MNIST dataset\n",
    "    train_dataset = torchvision.datasets.MNIST(root='/',\n",
    "                                                train=True,\n",
    "                                                download=True,\n",
    "                                                transform=transforms.ToTensor(),\n",
    "                                                target_transform = lambda y: np.array(list(np.binary_repr(y).zfill(4))).astype(np.float32))\n",
    "\n",
    "    test_dataset = torchvision.datasets.MNIST(root='/',\n",
    "                                                train=False,\n",
    "                                                download=True,\n",
    "                                                transform=transforms.ToTensor(),\n",
    "                                                target_transform = lambda y: np.array(list(np.binary_repr(y).zfill(4))).astype(np.float32))\n",
    "    # Data loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                            batch_size=64,\n",
    "                                            shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=64,\n",
    "                                            shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, num_epochs):\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for step, (images, labels) in enumerate(train_loader):\n",
    "            if model.type == 'MLP':\n",
    "                images = images.reshape(-1, 28 * 28)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step + 1) % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, step + 1, total_step, loss.item()))\n",
    "\n",
    "def test(test_loader, model):\n",
    "    # Test the model\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            if model.type == 'MLP':\n",
    "                images = images.reshape(-1, 28 * 28)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            predicted = np.abs(np.round(outputs.numpy()))\n",
    "            i = 0\n",
    "            for answer in labels.numpy():\n",
    "                if (predicted[i] == answer).all():\n",
    "                    correct += 1\n",
    "                i += 1\n",
    "            total += labels.size(0)\n",
    "            #correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0d2f2-a060-483c-8886-b14320d2445c",
   "metadata": {},
   "source": [
    "# one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3144275-a81d-4664-8ceb-b8efd94b2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = create_dataloader_one_hot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcb4613a-b1b8-4325-9a94-b997cf853773",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet_ont_hot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd3d79c-441f-493a-894a-9a67ea18396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdb4c080-49cc-4996-8031-95c6275e8b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/938], Loss: 0.0526\n",
      "Epoch [1/5], Step [200/938], Loss: 0.0462\n",
      "Epoch [1/5], Step [300/938], Loss: 0.0391\n",
      "Epoch [1/5], Step [400/938], Loss: 0.0357\n",
      "Epoch [1/5], Step [500/938], Loss: 0.0312\n",
      "Epoch [1/5], Step [600/938], Loss: 0.0257\n",
      "Epoch [1/5], Step [700/938], Loss: 0.0288\n",
      "Epoch [1/5], Step [800/938], Loss: 0.0309\n",
      "Epoch [1/5], Step [900/938], Loss: 0.0253\n",
      "Epoch [2/5], Step [100/938], Loss: 0.0258\n",
      "Epoch [2/5], Step [200/938], Loss: 0.0239\n",
      "Epoch [2/5], Step [300/938], Loss: 0.0173\n",
      "Epoch [2/5], Step [400/938], Loss: 0.0280\n",
      "Epoch [2/5], Step [500/938], Loss: 0.0229\n",
      "Epoch [2/5], Step [600/938], Loss: 0.0205\n",
      "Epoch [2/5], Step [700/938], Loss: 0.0175\n",
      "Epoch [2/5], Step [800/938], Loss: 0.0217\n",
      "Epoch [2/5], Step [900/938], Loss: 0.0265\n",
      "Epoch [3/5], Step [100/938], Loss: 0.0174\n",
      "Epoch [3/5], Step [200/938], Loss: 0.0213\n",
      "Epoch [3/5], Step [300/938], Loss: 0.0200\n",
      "Epoch [3/5], Step [400/938], Loss: 0.0188\n",
      "Epoch [3/5], Step [500/938], Loss: 0.0198\n",
      "Epoch [3/5], Step [600/938], Loss: 0.0210\n",
      "Epoch [3/5], Step [700/938], Loss: 0.0257\n",
      "Epoch [3/5], Step [800/938], Loss: 0.0157\n",
      "Epoch [3/5], Step [900/938], Loss: 0.0170\n",
      "Epoch [4/5], Step [100/938], Loss: 0.0212\n",
      "Epoch [4/5], Step [200/938], Loss: 0.0219\n",
      "Epoch [4/5], Step [300/938], Loss: 0.0224\n",
      "Epoch [4/5], Step [400/938], Loss: 0.0249\n",
      "Epoch [4/5], Step [500/938], Loss: 0.0206\n",
      "Epoch [4/5], Step [600/938], Loss: 0.0272\n",
      "Epoch [4/5], Step [700/938], Loss: 0.0187\n",
      "Epoch [4/5], Step [800/938], Loss: 0.0219\n",
      "Epoch [4/5], Step [900/938], Loss: 0.0181\n",
      "Epoch [5/5], Step [100/938], Loss: 0.0171\n",
      "Epoch [5/5], Step [200/938], Loss: 0.0143\n",
      "Epoch [5/5], Step [300/938], Loss: 0.0167\n",
      "Epoch [5/5], Step [400/938], Loss: 0.0194\n",
      "Epoch [5/5], Step [500/938], Loss: 0.0197\n",
      "Epoch [5/5], Step [600/938], Loss: 0.0153\n",
      "Epoch [5/5], Step [700/938], Loss: 0.0211\n",
      "Epoch [5/5], Step [800/938], Loss: 0.0153\n",
      "Epoch [5/5], Step [900/938], Loss: 0.0184\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, model, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26e98739-3dfe-449f-b11d-433d189e280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 86.66 %\n"
     ]
    }
   ],
   "source": [
    "test(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b419dc-c9e6-4125-bd72-aca88aecd42b",
   "metadata": {},
   "source": [
    "# binary_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f17e031-79fd-4a58-a6e2-96167478c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = create_dataloader_binary_encoding()\n",
    "model = NeuralNet_binary_encoding()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86bf3a0-89a2-4449-a4e3-b92ce3af4f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/938], Loss: 0.0945\n",
      "Epoch [1/5], Step [200/938], Loss: 0.0814\n",
      "Epoch [1/5], Step [300/938], Loss: 0.0683\n",
      "Epoch [1/5], Step [400/938], Loss: 0.0758\n",
      "Epoch [1/5], Step [500/938], Loss: 0.0676\n",
      "Epoch [1/5], Step [600/938], Loss: 0.0617\n",
      "Epoch [1/5], Step [700/938], Loss: 0.0522\n",
      "Epoch [1/5], Step [800/938], Loss: 0.0473\n",
      "Epoch [1/5], Step [900/938], Loss: 0.0545\n",
      "Epoch [2/5], Step [100/938], Loss: 0.0665\n",
      "Epoch [2/5], Step [200/938], Loss: 0.0547\n",
      "Epoch [2/5], Step [300/938], Loss: 0.0420\n",
      "Epoch [2/5], Step [400/938], Loss: 0.0476\n",
      "Epoch [2/5], Step [500/938], Loss: 0.0555\n",
      "Epoch [2/5], Step [600/938], Loss: 0.0489\n",
      "Epoch [2/5], Step [700/938], Loss: 0.0362\n",
      "Epoch [2/5], Step [800/938], Loss: 0.0638\n",
      "Epoch [2/5], Step [900/938], Loss: 0.0610\n",
      "Epoch [3/5], Step [100/938], Loss: 0.0565\n",
      "Epoch [3/5], Step [200/938], Loss: 0.0400\n",
      "Epoch [3/5], Step [300/938], Loss: 0.0532\n",
      "Epoch [3/5], Step [400/938], Loss: 0.0452\n",
      "Epoch [3/5], Step [500/938], Loss: 0.0501\n",
      "Epoch [3/5], Step [600/938], Loss: 0.0499\n",
      "Epoch [3/5], Step [700/938], Loss: 0.0451\n",
      "Epoch [3/5], Step [800/938], Loss: 0.0540\n",
      "Epoch [3/5], Step [900/938], Loss: 0.0449\n",
      "Epoch [4/5], Step [100/938], Loss: 0.0374\n",
      "Epoch [4/5], Step [200/938], Loss: 0.0357\n",
      "Epoch [4/5], Step [300/938], Loss: 0.0369\n",
      "Epoch [4/5], Step [400/938], Loss: 0.0542\n",
      "Epoch [4/5], Step [500/938], Loss: 0.0392\n",
      "Epoch [4/5], Step [600/938], Loss: 0.0459\n",
      "Epoch [4/5], Step [700/938], Loss: 0.0431\n",
      "Epoch [4/5], Step [800/938], Loss: 0.0482\n",
      "Epoch [4/5], Step [900/938], Loss: 0.0463\n",
      "Epoch [5/5], Step [100/938], Loss: 0.0564\n",
      "Epoch [5/5], Step [200/938], Loss: 0.0379\n",
      "Epoch [5/5], Step [300/938], Loss: 0.0332\n",
      "Epoch [5/5], Step [400/938], Loss: 0.0577\n",
      "Epoch [5/5], Step [500/938], Loss: 0.0465\n",
      "Epoch [5/5], Step [600/938], Loss: 0.0541\n",
      "Epoch [5/5], Step [700/938], Loss: 0.0385\n",
      "Epoch [5/5], Step [800/938], Loss: 0.0325\n",
      "Epoch [5/5], Step [900/938], Loss: 0.0357\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, model, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "247cca79-4429-4aad-809f-2fc85e352ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 88.68 %\n"
     ]
    }
   ],
   "source": [
    "test(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4661d0-e76d-4913-a81f-08d3e09232be",
   "metadata": {},
   "source": [
    "# binary_encoding with one more layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b2fc8b7-ad44-4845-84e6-e16e1046c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = create_dataloader_binary_encoding()\n",
    "model = NeuralNet_binary_encoding_one_more_layer()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d2c6eb4-4f57-4682-a02c-53af7a62f9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/938], Loss: 0.2254\n",
      "Epoch [1/5], Step [200/938], Loss: 0.1393\n",
      "Epoch [1/5], Step [300/938], Loss: 0.1193\n",
      "Epoch [1/5], Step [400/938], Loss: 0.1005\n",
      "Epoch [1/5], Step [500/938], Loss: 0.0902\n",
      "Epoch [1/5], Step [600/938], Loss: 0.0924\n",
      "Epoch [1/5], Step [700/938], Loss: 0.0932\n",
      "Epoch [1/5], Step [800/938], Loss: 0.0812\n",
      "Epoch [1/5], Step [900/938], Loss: 0.0833\n",
      "Epoch [2/5], Step [100/938], Loss: 0.0585\n",
      "Epoch [2/5], Step [200/938], Loss: 0.0930\n",
      "Epoch [2/5], Step [300/938], Loss: 0.0721\n",
      "Epoch [2/5], Step [400/938], Loss: 0.0691\n",
      "Epoch [2/5], Step [500/938], Loss: 0.0789\n",
      "Epoch [2/5], Step [600/938], Loss: 0.0779\n",
      "Epoch [2/5], Step [700/938], Loss: 0.0872\n",
      "Epoch [2/5], Step [800/938], Loss: 0.0663\n",
      "Epoch [2/5], Step [900/938], Loss: 0.0678\n",
      "Epoch [3/5], Step [100/938], Loss: 0.0669\n",
      "Epoch [3/5], Step [200/938], Loss: 0.0752\n",
      "Epoch [3/5], Step [300/938], Loss: 0.0831\n",
      "Epoch [3/5], Step [400/938], Loss: 0.0642\n",
      "Epoch [3/5], Step [500/938], Loss: 0.0581\n",
      "Epoch [3/5], Step [600/938], Loss: 0.0646\n",
      "Epoch [3/5], Step [700/938], Loss: 0.0486\n",
      "Epoch [3/5], Step [800/938], Loss: 0.0642\n",
      "Epoch [3/5], Step [900/938], Loss: 0.0694\n",
      "Epoch [4/5], Step [100/938], Loss: 0.0655\n",
      "Epoch [4/5], Step [200/938], Loss: 0.0626\n",
      "Epoch [4/5], Step [300/938], Loss: 0.0593\n",
      "Epoch [4/5], Step [400/938], Loss: 0.0661\n",
      "Epoch [4/5], Step [500/938], Loss: 0.0724\n",
      "Epoch [4/5], Step [600/938], Loss: 0.0532\n",
      "Epoch [4/5], Step [700/938], Loss: 0.0517\n",
      "Epoch [4/5], Step [800/938], Loss: 0.0671\n",
      "Epoch [4/5], Step [900/938], Loss: 0.0693\n",
      "Epoch [5/5], Step [100/938], Loss: 0.0654\n",
      "Epoch [5/5], Step [200/938], Loss: 0.0509\n",
      "Epoch [5/5], Step [300/938], Loss: 0.0576\n",
      "Epoch [5/5], Step [400/938], Loss: 0.0513\n",
      "Epoch [5/5], Step [500/938], Loss: 0.0611\n",
      "Epoch [5/5], Step [600/938], Loss: 0.0677\n",
      "Epoch [5/5], Step [700/938], Loss: 0.0492\n",
      "Epoch [5/5], Step [800/938], Loss: 0.0528\n",
      "Epoch [5/5], Step [900/938], Loss: 0.0537\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, model, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae0b68df-1d65-46f0-8c51-59466e34f627",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 86.31 %\n"
     ]
    }
   ],
   "source": [
    "test(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6145efaf-f3e1-441e-97f1-e788efb30768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca7e17-ee5a-4ec5-b705-3962b60b1f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c5a1a7-ce36-4245-b6e7-09010d8156a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3c581d-c91c-441b-a14c-f7be2b5923ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e49e83-ff5d-4ed7-ad30-326d57008395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c370ca46-ec14-4f55-9689-26e4e3b53d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d87b1-1469-4063-a922-47e9604a6fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a649a-42f3-4145-afe4-568d4d1781be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
